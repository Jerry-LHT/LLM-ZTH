{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "$$\n",
    "\\mathrm{Attention}(Q, K, V) = \\mathrm{Softmax}!\\left( \\frac{QK^{\\mathsf T}}{\\sqrt{d_k}} + M \\right)V\n",
    "$$\n",
    "\n",
    "**Self-Attention（自注意力）** 是大型语言模型（LLM）中的核心机制。\n",
    "它允许序列中每个 token 与同一序列中的其他 token 建立依赖关系。\n",
    "以下以输入文本 “a fluffy blue creature” 为例说明计算过程，并标注各个张量的维度（含 batch size）。\n",
    "\n",
    "---\n",
    "\n",
    "### 计算 Q、K、V 矩阵\n",
    "\n",
    "输入张量：\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{B \\times n \\times d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $B$：batch size（批量大小）\n",
    "* $n$：序列长度（例如 4）\n",
    "* $d_{\\text{model}}$：模型的嵌入维度（例如 12288）\n",
    "\n",
    "对输入进行线性变换， 作用为将输入映射到新的张量：\n",
    "\n",
    "$$\n",
    "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
    "$$\n",
    "\n",
    "权重矩阵和结果的维度为：\n",
    "\n",
    "$$\n",
    "W_Q, W_K, W_V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, \\qquad Q, K, V \\in \\mathbb{R}^{B \\times n \\times d_k}\n",
    "$$\n",
    "\n",
    "通常 $d_k \\ll d_{\\text{model}}$，以降低计算复杂度。\n",
    "\n",
    "语义示例：“creature” 的 $Q$ 向量偏向寻找形容词，“fluffy” 的 $K$ 向量则偏向标识自身为形容词。\n",
    "\n",
    "---\n",
    "\n",
    "### 计算注意力分数（Attention Score）\n",
    "\n",
    "对每个 batch 独立计算注意力分数：\n",
    "\n",
    "$$\n",
    "S[b] = \\frac{Q[b] K[b]^{\\mathsf T}}{\\sqrt{d_k}}, \\quad b = 1, \\dots, B\n",
    "$$\n",
    "\n",
    "整体张量：\n",
    "\n",
    "$$\n",
    "S \\in \\mathbb{R}^{B \\times n \\times n}\n",
    "$$\n",
    "\n",
    "每个元素 $S_{b, i, j}$ 表示第 $b$ 个样本中第 $i$ 个 token 的 query 与第 $j$ 个 token 的 key 的匹配程度。\n",
    "\n",
    "除以 $\\sqrt{d_k}$ 可以防止数值过大，避免 Softmax 输出饱和导致梯度消失。\n",
    "\n",
    "---\n",
    "\n",
    "### Softmax 归一化\n",
    "\n",
    "对每个 batch 的每个 query（第 $i$ 行）在 key 维度上执行 Softmax：\n",
    "\n",
    "$$\n",
    "A[b] = \\mathrm{Softmax}(S[b]), \\quad A \\in \\mathbb{R}^{B \\times n \\times n}\n",
    "$$\n",
    "\n",
    "保证每行的权重和为 1：\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{n} A_{b, i, j} = 1, \\quad \\forall b, i\n",
    "$$\n",
    "\n",
    "示例：\n",
    "“creature” 的注意力权重中，“fluffy” 和 “blue” 的概率较高，“a” 的权重较低。\n",
    "\n",
    "---\n",
    "\n",
    "### 加权求和得到上下文表示\n",
    "\n",
    "利用注意力权重加权求和 Value 向量：\n",
    "\n",
    "$$\n",
    "O[b] = A[b] V[b], \\quad b = 1, \\dots, B\n",
    "$$\n",
    "\n",
    "输出张量：\n",
    "\n",
    "$$\n",
    "O \\in \\mathbb{R}^{B \\times n \\times d_k}\n",
    "$$\n",
    "\n",
    "每个 $O_{b, i, :}$ 是第 $b$ 个样本中第 $i$ 个 token 的上下文表示，即对所有 $V_{b, j, :}$ 的加权和（权重为 $A_{b, i, j}$）。\n",
    "\n",
    "示例：“creature” 的输出向量融合了 “fluffy”（毛茸茸）和 “blue”（蓝色）的语义，从“通用生物”变为“毛茸茸的蓝色生物”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/handonLLM/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        # 将d_model映射为d_k/d_v\n",
    "        self.W_q = nn.Linear(self.d_model, self.d_k)\n",
    "        self.W_k = nn.Linear(self.d_model, self.d_k)\n",
    "        self.W_v = nn.Linear(self.d_model, self.d_v)\n",
    "\n",
    "        # 需将d_v映射回d_model\n",
    "        self.output_proj = nn.Linear(self.d_v, self.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (batch_size, seq_len, d_model)\n",
    "\n",
    "        # _ = x @ W\n",
    "        # (batch_size, seq_len, d_model) ->  (batch_size, seq_len, d_k/d_v)\n",
    "        Q = self.W_q(x) \n",
    "        K = self.W_k(x) \n",
    "        V = self.W_v(x) \n",
    "\n",
    "        # (batch_size, seq_len, d_k/d_v)->(batch_size, d_k/d_v, seq_len)\n",
    "        # (batch_size, seq_len, d_k/d_v) @ (batch_size, d_k/d_v, seq_len)\n",
    "        # 衡量“当前Token的Q”与“其他所有Token的K”的匹配度（关联度） \n",
    "        attention_score = (torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32, device=x.device)))\n",
    "\n",
    "        # (batch_size, seq_len, seq_len) 需要按列softmax -> dim=-1\n",
    "        attention_weight = torch.softmax(attention_score, dim=-1)\n",
    "\n",
    "        # 通过attention_weight计算出Value\n",
    "        # (batch_size, seq_len, seq_len) @ (batch_size, seq_len, d_v)\n",
    "        output = attention_weight @ V\n",
    "\n",
    "        # 通过输出线性层映射回d_model\n",
    "        output = self.output_proj(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2391, -0.0947,  0.2341, -0.3469],\n",
      "         [ 0.2393, -0.0945,  0.2352, -0.3482]],\n",
      "\n",
      "        [[ 0.2411, -0.0467,  0.3459, -0.4229],\n",
      "         [ 0.2415, -0.0468,  0.3464, -0.4239]],\n",
      "\n",
      "        [[ 0.2192, -0.1058,  0.2581, -0.3823],\n",
      "         [ 0.2189, -0.1054,  0.2585, -0.3822]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, 2, 4)\n",
    "net = SelfAttention(d_model=4, d_k=5, d_v=3)\n",
    "print(net(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention 优化\n",
    "### Masking（掩码）防止信息泄露\n",
    "\n",
    "在自回归任务中，模型不应看到未来的 token。\n",
    "因此在 Softmax 前添加掩码矩阵 $M$：\n",
    "\n",
    "$$\n",
    "A = \\mathrm{Softmax}\\left( \\frac{QK^{\\mathsf{T}}}{\\sqrt{d_k}} + M \\right)\n",
    "$$\n",
    "\n",
    "其中掩码定义为：\n",
    "\n",
    "$$\n",
    "M_{i,j} = \n",
    "\\begin{cases} \n",
    "0, & j \\le i, \\\\\n",
    "-\\infty, & j > i.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Softmax 后，$M_{i,j} = -\\infty$ 的位置权重自动变为 0。\n",
    "\n",
    "此外，还可使用 **Padding Mask**，对 `[PAD]` 等填充 token 的注意力分数设为 $-\\infty$，避免无效位置干扰。\n",
    "\n",
    "---\n",
    "\n",
    "### Dropout（自注意力中的随机丢弃）\n",
    "\n",
    "**作用**  \n",
    "- 防止过拟合，增强模型泛化能力；\n",
    "- 在训练中随机丢弃部分注意力连接。\n",
    "\n",
    "**在注意力权重上的 Dropout**  \n",
    "\n",
    "在 Softmax 后对注意力矩阵应用 Dropout：\n",
    "\n",
    "$$\n",
    "A' = \\mathrm{Dropout}(A; p), \\qquad A \\in \\mathbb{R}^{B \\times h \\times n \\times n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A'_{b,i,j} =\n",
    "\\begin{cases}\n",
    "\\dfrac{A_{b,i,j}}{1-p}, & \\text{若位置 }(b,i,j)\\text{ 未被丢弃}, \\\\\n",
    "0, & \\text{若位置 }(b,i,j)\\text{ 被丢弃}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Dropout 生成与 $A$ 同形状的掩码 $M_{\\text{drop}} \\in \\{0,1\\}^{B \\times h \\times n \\times n}$，并对未丢弃部分按 $1/(1-p)$ 放缩（**Inverted Dropout**）。\n",
    "\n",
    "输出再与 $V$ 相乘：\n",
    "\n",
    "$$\n",
    "O = A'V, \\qquad O \\in \\mathbb{R}^{B \\times h \\times n \\times d_{\\text{head}}}\n",
    "$$\n",
    "\n",
    "**训练与推理阶段的区别**  \n",
    "- 训练：随机丢弃部分注意力连接，保留部分按 $1/(1-p)$ 放缩 \n",
    "- 推理：禁用 Dropout，使用完整注意力权重矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, dropout_rate=0.1) -> None:\n",
    "        super().__init__()\n",
    "        # 输入的维度\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # 将d_model映射为d_k/d_v\n",
    "        self.W_q = nn.Linear(self.d_model, self.d_k)\n",
    "        self.W_k = nn.Linear(self.d_model, self.d_k)\n",
    "        self.W_v = nn.Linear(self.d_model, self.d_v)\n",
    "\n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "        # 需将d_v映射回d_model\n",
    "        self.output_proj = nn.Linear(self.d_v, self.d_model)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # x.shape = (batch_size, seq_len, d_model)\n",
    "\n",
    "        # _ = x @ W\n",
    "        # (batch_size, seq_len, d_model) ->  (batch_size, seq_len, d_k/d_v)\n",
    "        Q = self.W_q(x) \n",
    "        K = self.W_k(x) \n",
    "        V = self.W_v(x) \n",
    "\n",
    "        # (batch_size, seq_len, d_k/d_v)->(batch_size, d_k/d_v, seq_len)\n",
    "        # (batch_size, seq_len, d_k/d_v) @ (batch_size, d_k/d_v, seq_len)\n",
    "        # 衡量“当前Token的Q”与“其他所有Token的K”的匹配度（关联度） \n",
    "        attention_score = (torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32, device=x.device)))\n",
    "        # mask\n",
    "        # attention_mask shape is (batch, seq)\n",
    "        if attention_mask is not None:\n",
    "            attention_score = attention_score.masked_fill(attention_mask==0,  float(\"-inf\"))\n",
    "\n",
    "        # (batch_size, seq_len, seq_len) 需要按列softmax -> dim=-1\n",
    "        attention_weight = torch.softmax(attention_score, dim=-1)\n",
    "\n",
    "        # 传入矩阵，dropout\n",
    "        attention_weight = self.dropout(attention_weight)\n",
    "\n",
    "        # 通过attention_weight计算出Value\n",
    "        # (batch_size, seq_len, seq_len) @ (batch_size, seq_len, d_v)\n",
    "        output = attention_weight @ V\n",
    "\n",
    "        # 通过输出线性层映射回d_model\n",
    "        output = self.output_proj(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2835, 0.5654],\n",
      "         [0.7871, 0.9491],\n",
      "         [0.5740, 0.2463],\n",
      "         [0.4636, 0.5301]],\n",
      "\n",
      "        [[0.9686, 0.5447],\n",
      "         [0.3322, 0.1100],\n",
      "         [0.5974, 0.2093],\n",
      "         [0.6519, 0.1966]],\n",
      "\n",
      "        [[0.0341, 0.8704],\n",
      "         [0.3002, 0.9153],\n",
      "         [0.5888, 0.3706],\n",
      "         [0.0245, 0.0150]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4857, -1.0564],\n",
       "         [ 0.4382, -0.8936],\n",
       "         [ 0.4859, -1.0542],\n",
       "         [ 0.4858, -1.0556]],\n",
       "\n",
       "        [[ 0.5426, -1.0690],\n",
       "         [ 0.5410, -1.0720],\n",
       "         [ 0.5422, -1.0698],\n",
       "         [ 0.5426, -1.0690]],\n",
       "\n",
       "        [[ 0.3721, -0.3863],\n",
       "         [ 0.4744, -0.8330],\n",
       "         [ 0.4744, -0.8330],\n",
       "         [ 0.4744, -0.8330]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(3, 4, 2)\n",
    "b = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "mask = b.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "\n",
    "net = SelfAttentionV2(2, 2, 2)\n",
    "print(X)\n",
    "net(X, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多头注意力（Multi-Head Attention）\n",
    "\n",
    "多头注意力机制（**Multi-Head Attention, MHA**）最早由 *Vaswani et al.* 在论文 **Attention Is All You Need (2017)** 中提出，用于增强模型捕捉多种依赖关系的能力。\n",
    "\n",
    "单头注意力（Single-Head Attention）只能学习一种类型的上下文关系（例如形容词与名词之间的依赖），  \n",
    "而多头注意力通过并行多个独立的注意力头，使模型能够同时学习语法、语义以及长距离指代等不同类型的依赖。\n",
    "\n",
    "---\n",
    "\n",
    "### 数学定义\n",
    "\n",
    "多头注意力的整体定义为：\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^O\n",
    "$$\n",
    "\n",
    "其中每个注意力头的计算为：\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)\n",
    "$$\n",
    "\n",
    "单个注意力函数（Scaled Dot-Product Attention）的形式为：\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 矩阵维度关系\n",
    "\n",
    "设：\n",
    "\n",
    "- 批量大小为 **B**，序列长度为 **n**；\n",
    "- 模型隐层总维度为 **d_model**；\n",
    "- 注意力头的数量为 **h**；\n",
    "- 每个注意力头的特征维度为  \n",
    "  $d_k = d_v = d_{\\text{model}} / h$。\n",
    "\n",
    "则各矩阵的维度关系如下：\n",
    "\n",
    "1. **输入矩阵：**\n",
    "\n",
    "   $$\n",
    "   Q, K, V \\in \\mathbb{R}^{B \\times n \\times d_{\\text{model}}}\n",
    "   $$\n",
    "\n",
    "2. **每个注意力头的线性映射参数：**\n",
    "\n",
    "   $$\n",
    "   W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}\n",
    "   $$\n",
    "\n",
    "   投影后：\n",
    "\n",
    "   $$\n",
    "   Q_i = Q W_i^Q,\\quad K_i = K W_i^K,\\quad V_i = V W_i^V\n",
    "   $$\n",
    "\n",
    "   此时：\n",
    "\n",
    "   $$\n",
    "   Q_i, K_i, V_i \\in \\mathbb{R}^{B \\times n \\times d_k}\n",
    "   $$\n",
    "\n",
    "3. **每个头的输出：**\n",
    "\n",
    "   $$\n",
    "   O_i = \\text{Softmax}\\left(\\frac{Q_i K_i^\\top}{\\sqrt{d_k}}\\right)V_i\n",
    "   $$\n",
    "\n",
    "   输出维度：\n",
    "\n",
    "   $$\n",
    "   O_i \\in \\mathbb{R}^{B \\times n \\times d_v}\n",
    "   $$\n",
    "\n",
    "4. **拼接所有头的结果：**\n",
    "\n",
    "   $$\n",
    "   O_{\\text{concat}} = [O_1; O_2; \\dots; O_h] \\in \\mathbb{R}^{B \\times n \\times (h \\cdot d_v)} = \\mathbb{R}^{B \\times n \\times d_{\\text{model}}}\n",
    "   $$\n",
    "\n",
    "5. **最终线性变换：**\n",
    "\n",
    "   - 线性映射矩阵：  \n",
    "     $$\n",
    "     W^O \\in \\mathbb{R}^{(h \\cdot d_v) \\times d_{\\text{model}}}\n",
    "     $$\n",
    "   - 输出为：  \n",
    "     $$\n",
    "     O_{\\text{final}} = O_{\\text{concat}} W^O \\in \\mathbb{R}^{B \\times n \\times d_{\\text{model}}}\n",
    "     $$\n",
    "\n",
    "---\n",
    "\n",
    "### 计算流程\n",
    "\n",
    "1. **分头线性映射：**  \n",
    "   将输入的 \\( Q, K, V \\) 分别投影为 \\( h \\) 个子空间：\n",
    "\n",
    "   $$\n",
    "   Q_i = Q W_i^Q, \\quad K_i = K W_i^K, \\quad V_i = V W_i^V\n",
    "   $$\n",
    "\n",
    "2. **各头独立计算注意力：**  \n",
    "   对每个头并行计算缩放点积注意力：\n",
    "\n",
    "   $$\n",
    "   O_i = \\text{Softmax}\\left(\\frac{Q_i K_i^\\top}{\\sqrt{d_k}}\\right)V_i\n",
    "   $$\n",
    "\n",
    "3. **拼接与线性变换：**  \n",
    "   拼接所有头的输出，再经线性层恢复至原始维度：\n",
    "\n",
    "   $$\n",
    "   O_{\\text{final}} = [O_1; O_2; \\dots; O_h] W^O\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### 多头注意力的优势\n",
    "\n",
    "1. **多层次依赖建模：**  \n",
    "   不同注意力头可捕捉不同类型的语言特征：\n",
    "   - 某些头学习局部语法结构（如冠词–名词搭配）；\n",
    "   - 另一些头学习长距离依赖（如代词与前文实体的对应）。\n",
    "\n",
    "2. **增强鲁棒性：**  \n",
    "   多个头的结果互补，降低单一注意力机制的偏差风险。\n",
    "\n",
    "3. **可解释性：**  \n",
    "   实证研究发现，不同注意力头在语法、语义和实体关系等方面表现出特定的专门化模式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # d_model == d_k == d_v\n",
    "    def __init__(self, d_model, head_num, dropout_rate=0.1) -> None:\n",
    "        super().__init__()\n",
    "        # 输入的维度\n",
    "        self.d_model = d_model\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # head_num and head_dim\n",
    "        self.head_num = head_num\n",
    "        self.head_dim = self.d_model // head_num\n",
    "        # 将d_model映射为d_k/d_v\n",
    "        self.W_q = nn.Linear(self.d_model, self.d_model)\n",
    "        self.W_k = nn.Linear(self.d_model, self.d_model)\n",
    "        self.W_v = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "        # 需将d_v映射回d_model\n",
    "        self.output_proj = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "        # 预先计算缩放因子，避免每次forward都创建tensor\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self, query, key=None, value=None, attention_mask=None):\n",
    "        # 兼容 self-attention 和 cross-attention\n",
    "        if key is None:\n",
    "            key = query\n",
    "        if value is None:\n",
    "            value = query\n",
    "\n",
    "        # x.shape = (batch_size, seq_len, d_model)\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "\n",
    "        # _ = x @ W\n",
    "        # (batch_size, seq_len, d_model) ->  (batch_size, seq_len, d_k/d_v)\n",
    "        Q = self.W_q(query) \n",
    "        K = self.W_k(key) \n",
    "        V = self.W_v(value) \n",
    "\n",
    "        # shape: （batch_size, seq_len, d_model=head_num * head_dim) ->（batch_size, num_head, seq_len, head_dim)\n",
    "        q_state = Q.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        k_state = K.view(batch_size, K.size(1), self.head_num, self.head_dim).transpose(1, 2)\n",
    "        v_state = V.view(batch_size, V.size(1), self.head_num, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # (batch_size, head_num, seq_len, head_dim)->(batch_size, head_num, head_dim, seq_len)\n",
    "        # (batch_size, head_num, seq_len, head_dim)->(batch_size, head_num, seq_len, seq_len)\n",
    "        attention_score = torch.matmul(q_state, k_state.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # mask\n",
    "        if attention_mask is not None:\n",
    "            # 保证mask维度正确 (batch_size, 1, 1, seq_len)\n",
    "            if attention_mask.dim() == 2:\n",
    "                attention_mask = attention_mask[:, None, None, :]\n",
    "            attention_score = attention_score.masked_fill(attention_mask == 0, float(\"-inf\"))\n",
    "\n",
    "        # (batch_size, head_num, seq_len, seq_len) 需要按列softmax -> dim=-1\n",
    "        attention_weight = torch.softmax(attention_score, dim=-1)\n",
    "\n",
    "        # 传入矩阵，dropout\n",
    "        attention_weight = self.dropout(attention_weight)\n",
    "\n",
    "        # 通过attention_weight计算出Value\n",
    "        # (batch_size, head_num, seq_len, seq_len) @ (batch_size, head_num, seq_len, d_v)\n",
    "        output = torch.matmul(attention_weight, v_state)\n",
    "\n",
    "        # 多头聚合\n",
    "        # 重新变成 (batch, seq_len, num_head, head_dim)\n",
    "        # 这里的 contiguous() 是相当于返回一个连续内存的 tensor，一般用了 permute/tranpose 都要这么操作\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, -1)\n",
    "\n",
    "        # 通过输出线性层映射回d_model\n",
    "        output = self.output_proj(output)\n",
    "\n",
    "        # 返回输出与注意力权重（方便可视化或调试）\n",
    "        return output, attention_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [0, 1],\n",
    "            [0, 0],\n",
    "            [1, 0],\n",
    "        ]\n",
    "    )\n",
    "    .unsqueeze(1)\n",
    "    .unsqueeze(2)\n",
    "    .expand(3, 8, 2, 2)\n",
    ")\n",
    "\n",
    "x = torch.rand(3, 2, 128)\n",
    "net = MultiHeadAttention(128, 8)\n",
    "net(x, attention_mask=attention_mask)[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handonLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
